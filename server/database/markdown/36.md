>最近在网上看到了一个小说，感觉挺好看的，在线阅读的网站广告比较多，想下载下来，结果找半天也没个全本下载的。于是想到用爬虫自己来实现，下面记录了我的实现过程。

### 使用nodegrass做网络请求

刚开始是使用`http`模块的`get`方法来做网络请求的，不过一直有中文乱码的问题，于是找到了`nodegrass`，使用比较简单：

```
var gs = require('nodegrass');
gs.get('章节列表地址', function (data) {
  filterHtml(data)
}, 'gbk');
```

### 使用cheerio做爬虫

这里使用`cheerio`来解析`html`，这个库对前端来说简直是如鱼得水，它具有和`JQuery`相似的语法，使用起来毫无障碍。

```
function filterHtml(html) {
    // 沿用JQuery风格，定义$
    var $ = cheerio.load(html);
    //遍历章节列表，获取所有章节的url
    $('#list a').each(function () {
        getNovelContent($(this).attr('href'), $(this).text())
    });
}
function getNovelContent(url, title) {
    //获取html中的小说内容并存入本地
    gs.get(url, function (data) {
        var $ = cheerio.load(data);
        //有时一次不能下载所有的章节，再执行一次，并过滤掉已经存在的
        if (!fse.pathExistsSync('./novel/' + title + '.txt')) {
            fse.outputFile('./novel/' + title + '.txt', title + '\n' + $('#content').text() + '\n', function (err) {
                if (err) {
                    console.log(err);
                    return;
                }
                console.log(title + '保存成功');
            });
        }
    }, 'gbk');
}
```


### 多个txt文件的合并

上面的方法有个问题，就是下载下来的每个章节都是一个txt文件，下面这个脚本可以将多个txt文件合并成一个文件。

```
::合并多个txt文件.bat
::将目录下的所有文件按照文件名顺序合并，保存为name.txt
copy *.txt name.txt
```